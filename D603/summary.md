# Course Summary
**D603: Machine Learning** 

D603 focuses on applying various machine learning models to analyze data and generate forecasts. It covers supervised learning (classification), unsupervised learning (clustering), and time series modeling. Learners will select appropriate models, prepare data, perform analysis, visualize findings, and make recommendations.
# Course Objectives 
WGU outlines the following competencies as a part of this class:
- **Recommends a Supervised Machine Learning Model:** The learner recommends a supervised machine learning model based on a comparison of model performance given a business problem.
- **Recommends an Unsupervised Machine Learning Model:** The learner recommends an unsupervised machine learning model based on a comparison of model performance given a business problem.
- **Applies Time Series Models:** The learner applies time series models in generating forecasts.
# Course Materials
This course uses Python or R for coding solutions. Code and project progression are managed via GitLab repositories for all tasks. Panopto video recordings are a consistent submission requirement for Task 1 and 2, while Task 3 requires a report in an interactive development environment (e.g., R Markdown, Jupyter Notebook) and its PDF/HTML output. Datasets like churn_clean.csv and medical_clean.csv are used across tasks.
# Practical Assessment(s) Overview & Files
- **Task 1:** Classification Data Mining Models
    - **Description:** Learners create a data mining report using a selected dataset (churn_clean.csv or medical_clean.csv). They propose a research question to be answered using one of the following classification methods: Random Forest, AdaBoost, or Gradient Boost. A data analysis goal is also defined. The task requires explaining how the chosen classification method analyzes the dataset (including expected outcomes) and listing/justifying chosen packages/libraries. Data preparation involves describing a preprocessing goal, identifying initial dataset variables (continuous/categorical), explaining preprocessing steps with code segments, and providing a cleaned dataset copy. The data is split into training, validation, and test datasets. An initial model is created using the training set, and a screenshot of metrics (accuracy, precision, recall, F1 score, AUC-ROC, confusion matrix) is provided. Hyperparameter tuning is performed on the validation dataset using k-fold cross validation to find an optimized model, with identification/justification of hyperparameters and a screenshot of the best ones. Predictions are made on the test dataset using the optimized model, and a screenshot of the same metrics is provided. The analysis is summarized by comparing metrics of the initial and optimized models, discussing results/implications, one limitation, and recommending a course of action.
  - **Output Files:** GitLab repository with committed code, a data mining report (document file) including descriptions, justifications, screenshots of metrics, and a cleaned dataset. A Panopto video recording demonstrating code functionality and programming environment is also required.
- **Task 2:** Clustering Techniques
    - **Description:** Learners create a data mining report using clustering techniques to identify groups of customers or patients with similar characteristics. They propose a research question using either k-means or hierarchical clustering (noting k-means requires continuous variables). A data analysis goal is defined. The task requires explaining the chosen clustering technique (how it analyzes data, expected outcomes), summarizing one assumption, and listing/justifying chosen packages/libraries. Data preparation involves describing a preprocessing goal, identifying initial dataset variables (continuous/categorical), explaining preprocessing steps with code segments, and providing a cleaned dataset copy. The optimal number of clusters in the dataset must be determined, and the method used described. The analysis is summarized by visualizing the clusters, explaining their quality (with screenshot), discussing results/implications, one limitation, and recommending a course of action.
  - **Output Files:** GitLab repository with committed code, a data mining report (document file) including descriptions, justifications, screenshots of cluster visualizations, and a cleaned dataset. A Panopto video recording demonstrating code functionality and programming environment is also required.
- **Task 3:** Time Series Modeling
    - **Description:** This task involves using time series modeling to analyze a chosen dataset, create visualizations, and generate forecasts. Learners summarize a research question to be answered using time series modeling and define objectives/goals. They summarize the assumptions of a time series model, including stationarity and autocorrelated data. Data cleaning steps include providing a line graph visualizing the time series realization, describing time step formatting (gaps, sequence length), evaluating stationarity, explaining data preparation steps (including training and test set split), and providing a cleaned dataset copy. Time series analysis involves reporting annotated findings with visualizations (trends, spectral density, decomposed time series, confirmation of lack of trends in residuals). An autoregressive integrated moving average (ARIMA) model that accounts for observed trend and seasonality is identified, and a forecast is performed using this model, with output and calculations provided. Findings are summarized by discussing analysis results (ARIMA model selection, prediction interval, forecast length justification, model evaluation/error metric), providing an annotated visualization of the forecast compared to the test set (with prediction line, confidence cone, labels), and recommending a course of action.
  - **Output Files:** GitLab repository with committed code. A report created in an industry-relevant interactive development environment (e.g., R Markdown document, Jupyter Notebook), including a PDF or HTML document of the executed notebook presentation.
